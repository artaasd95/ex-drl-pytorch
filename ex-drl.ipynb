{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "import torch.functional as F\n",
    "from models import *\n",
    "from scipy.optimize import minimize\n",
    "from torch.distributions import Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gpd_tail(psi_params, size=MTail):\n",
    "    \"\"\"Sample from GPD for tail distribution.\"\"\"\n",
    "    scale, shape = psi_params\n",
    "    return dist.GeneralizedPareto(scale=scale, concentration=shape).sample((size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_loss(quantiles, targets, N, MBody, MTail):\n",
    "    \"\"\"Quantile Regression Loss for EX-D4PG (Formula from step 12).\"\"\"\n",
    "    loss_body = torch.mean(torch.abs(quantiles[:MBody] - targets[:MBody]))\n",
    "    loss_tail = torch.mean(torch.abs(quantiles[-MTail:] - targets[-MTail:]))\n",
    "    return loss_body + loss_tail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi_mle(quantiles):\n",
    "    \"\"\"\n",
    "    Estimate GPD parameters (shape and scale) using Maximum Likelihood Estimation (MLE).\n",
    "    \n",
    "    Args:\n",
    "    quantiles (torch.Tensor): Selected quantiles (below u(s_double_prime, a_double_prime)).\n",
    "\n",
    "    Returns:\n",
    "    psi (tuple): Estimated GPD parameters (shape, scale).\n",
    "    \"\"\"\n",
    "    # Convert torch tensor to numpy for use with scipy.optimize\n",
    "    quantiles_np = quantiles.detach().cpu().numpy()\n",
    "\n",
    "    # Initial guess for shape and scale\n",
    "    initial_params = [0.1, 1.0]  # Starting points for shape and scale\n",
    "\n",
    "    # Log-Likelihood function for GPD\n",
    "    def gpd_neg_log_likelihood(params):\n",
    "        shape, scale = params\n",
    "        if scale <= 0:\n",
    "            return float('inf')  # Scale must be positive\n",
    "\n",
    "        # Negative log-likelihood for GPD\n",
    "        n = len(quantiles_np)\n",
    "        term1 = n * torch.log(torch.tensor(scale))\n",
    "        term2 = (1 + 1 / shape) * torch.sum(torch.log(1 + shape * quantiles_np / scale))\n",
    "        log_likelihood = -(term1 + term2)\n",
    "\n",
    "        return log_likelihood.item()\n",
    "\n",
    "    # Minimize the negative log-likelihood\n",
    "    result = minimize(gpd_neg_log_likelihood, initial_params, method='L-BFGS-B')\n",
    "\n",
    "    # The result contains the optimal shape and scale\n",
    "    shape_mle, scale_mle = result.x\n",
    "\n",
    "    return (shape_mle, scale_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32  # number of quantiles (formula from step 1)\n",
    "MTail = 10  # number of tail samples (step 3)\n",
    "beta = 0.2  # body proportion\n",
    "alpha = 1e-3  # learning rate (step 3)\n",
    "state_dim = 8  # example state dimension\n",
    "action_dim = 4  # example action dimension\n",
    "\n",
    "critic = Critic(state_dim, action_dim, N)\n",
    "target_critic = Critic(state_dim, action_dim, N)\n",
    "actor = Actor(state_dim, action_dim)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=alpha)\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update actor (line 14 in algorithm)\n",
    "def update_actor(s, a):\n",
    "    \"\"\"Update actor network to maximize VaR or CVaR (line 14).\"\"\"\n",
    "    # Maximize risk-adjusted reward\n",
    "    policy_loss = -critic(s, actor(s)).mean()  # Placeholder for risk metric VaR or CVaR\n",
    "\n",
    "    optimizer_actor.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer_actor.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update target distribution parameters (lines 15-19 in algorithm)\n",
    "def update_target(s, a, s_double_prime, a_double_prime, N):\n",
    "    \"\"\"Update target distribution (line 15-19).\"\"\"\n",
    "    # Sample new tail quantiles (line 16)\n",
    "    quantiles = critic(s_double_prime, a_double_prime)\n",
    "    tau = torch.arange(0, N) / N  # Formula for quantiles (line 17)\n",
    "    selected_quantiles = quantiles[quantiles < u(s_double_prime, a_double_prime)]\n",
    "\n",
    "    # Update GPD parameters using MLE (line 18)\n",
    "    psi_new = psi_mle(selected_quantiles)\n",
    "\n",
    "    return psi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_huber_loss(input, target, tau):\n",
    "    # Line 5: Compute the difference between the target and input.\n",
    "    diff = target - input\n",
    "    # Line 6: Use the quantile loss for both positive and negative errors.\n",
    "    loss = torch.where(diff > 0, tau * diff, (tau - 1) * diff)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_critic(critic_network, body_samples, tail_samples, \n",
    "                  target_body, target_tail, taus, \n",
    "                  M_body, M_tail, optimizer):\n",
    "    \n",
    "    # Line 11: Number of quantiles (N)\n",
    "    N = len(taus)\n",
    "    \n",
    "    # Line 13: Initialize QR loss to 0\n",
    "    qr_loss = 0.0\n",
    "\n",
    "    # Line 15: Loop over each quantile \\tau_n\n",
    "    for tau_idx in range(N):\n",
    "        tau = taus[tau_idx]  # \\tau_n for the current quantile\n",
    "\n",
    "        # (3) Line 19: Predicted quantile for body samples, \\theta_w^{\\tau_n}(s,a)\n",
    "        critic_prediction_body = critic_network(body_samples['state'], body_samples['action'])[:, tau_idx]\n",
    "        # (4) Line 21: Predicted quantile for tail samples, \\theta_w^{\\tau_n}(s,a)\n",
    "        critic_prediction_tail = critic_network(tail_samples['state'], tail_samples['action'])[:, tau_idx]\n",
    "\n",
    "        # (5) Line 23: Calculate quantile loss for body samples \\rho_{\\tau_n}(z_l^{\\text{Body}} - \\theta_w^{\\tau_n}(s,a))\n",
    "        loss_body = quantile_huber_loss(critic_prediction_body, target_body, tau)\n",
    "        loss_body = loss_body.mean()  # Mean over body samples\n",
    "\n",
    "        # (6) Line 27: Calculate quantile loss for tail samples \\rho_{\\tau_n}(z_k^{\\text{Tail}} - \\theta_w^{\\tau_n}(s,a))\n",
    "        loss_tail = quantile_huber_loss(critic_prediction_tail, target_tail, tau)\n",
    "        loss_tail = loss_tail.mean()  # Mean over tail samples\n",
    "\n",
    "        # (7) Line 31: Combine the two losses as per Equation 20\n",
    "        # \\frac{1}{M_{\\text{Body}}} and \\frac{1}{M_{\\text{Tail}}} are the sample weights\n",
    "        qr_loss += (1 / M_body) * loss_body + (1 / M_tail) * loss_tail\n",
    "\n",
    "    # (8) Line 35: Backpropagate the loss to update critic's parameters\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    qr_loss.backward()     # Backpropagate the QR loss\n",
    "    optimizer.step()       # Update the parameters\n",
    "\n",
    "    return qr_loss.item()  # Return the QR loss value for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example loop for a single episode, following the time loop (line 3 in the algorithm)\n",
    "for t in range(1000):  # Time loop (line 3)\n",
    "    s = torch.randn(state_dim)  # Example state\n",
    "    a = actor(s)  # Action from policy\n",
    "    r = torch.randn(1)  # Example reward\n",
    "    s_prime = torch.randn(state_dim)  # Example next state\n",
    "\n",
    "    # Threshold selection (line 4)\n",
    "    u = lambda s, a: torch.quantile(critic(s, a), 1 - beta)  # Placeholder quantile function\n",
    "\n",
    "    # Update critic (line 12)\n",
    "    update_critic(s, a, r, s_prime)\n",
    "\n",
    "    # Update actor (line 14)\n",
    "    update_actor(s, a)\n",
    "\n",
    "    # Update target distribution (lines 15-19)\n",
    "    psi_params = update_target(s, a, s_prime, actor(s_prime), N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
